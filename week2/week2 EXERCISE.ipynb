{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import json\n",
    "from IPython.display import Markdown, display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6256e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "\n",
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11736c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Let's break down the code snippet you provided."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Code Explanation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Components of the Code"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "1. **Set Comprehension**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "    - The part of the code `{book.get(\"author\") for book in books if book.get(\"author\")}` is a set comprehension."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "    - It iterates over `books`, which is expected to be a collection (like a list) of dictionaries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "    - For each `book` in `books`, `book.get(\"author\")` attempts to retrieve the value associated with the key `\"author\"` in the `book` dictionary."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "    - The condition `if book.get(\"author\")` ensures that only books with a non-empty author will be included in the resulting set."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "    - The use of a set comprehension means that duplicates will be eliminated automatically, and a set will be created containing unique authors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2. **`yield from`**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "    - The `yield from` expression is used within a generator function. It allows the generator to yield all values from another iterable."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "    - In this case, it yields each author from the set of unique authors generated by the set comprehension."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Purpose of the Code"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- The purpose of this line is to create a generator that yields unique authors from a list of book dictionaries, but only if the authors exist (i.e., are not `None` or empty strings)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- This is useful in contexts where you want to process or retrieve authors without duplicates, perhaps for listing or further processing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Example Use Case"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "If you have a list of books represented as dictionaries, like this:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "books = ["
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "    {\"title\": \"Book One\", \"author\": \"Author A\"},"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "    {\"title\": \"Book Two\", \"author\": \"Author B\"},"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "    {\"title\": \"Book Three\", \"author\": \"Author A\"},"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "    {\"title\": \"Book Four\", \"author\": None},"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "    {\"title\": \"Book Five\", \"author\": \"Author C\"},"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "When running the generator consuming the code snippet above, it would yield:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- \"Author A\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- \"Author B\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- \"Author C\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "This would result in a unique set of authors, where:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- \"Author A\" only appears once, despite being associated with two different books."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- The book without an author (`None`) is ignored."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why. Use markdowns.\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "# Get gpt-4o-mini to answer, with streaming\n",
    "openai = OpenAI()\n",
    "\n",
    "for chunk in openai.chat.completions.create(\n",
    "    model=MODEL_GPT,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that explains code.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    stream=True\n",
    "):\n",
    "    if chunk.choices[0].delta.content:\n",
    "        # Buffer to accumulate markdown output and print only on newlines\n",
    "        if 'markdown_buffer' not in globals():\n",
    "            markdown_buffer = \"\"\n",
    "        markdown_buffer += chunk.choices[0].delta.content\n",
    "        while \"\\n\" in markdown_buffer:\n",
    "            line, markdown_buffer = markdown_buffer.split(\"\\n\", 1)\n",
    "            display(Markdown(line))\n",
    "    # Print any remaining buffer at the end (outside the loop, after streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9f631e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Code Explanation**\n",
       "===============\n",
       "\n",
       "```python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "```\n",
       "\n",
       "This code uses a combination of Python features to extract authors' names from a list of books.\n",
       "\n",
       "### Breakdown:\n",
       "\n",
       "* `yield from`: This keyword is used to delegate the iteration over another iterable (in this case, a dictionary comprehension) to the caller.\n",
       "* `{book.get(\"author\") for book in books if book.get(\"author\")}`: This is a dictionary comprehension that generates a sequence of authors' names.\n",
       "\n",
       "### How it works:\n",
       "\n",
       "1. The comprehension iterates over each `book` in the `books` list.\n",
       "2. For each `book`, it checks if the \"author\" key exists and has a value using `book.get(\"author\")`. If it does, the author's name is included in the sequence.\n",
       "3. The resulting sequence of authors' names is then passed to the `yield from` statement.\n",
       "\n",
       "### Purpose:\n",
       "\n",
       "The purpose of this code is likely to yield each author's name as they are encountered while iterating over the list of books. This can be used in a generator function or other context where you want to process authors individually.\n",
       "\n",
       "### Use Cases:\n",
       "\n",
       "* Processing lists of books and extracting authors' names\n",
       "* Creating an iterator that yields individual authors' names\n",
       "\n",
       "### Example Usage:\n",
       "\n",
       "```python\n",
       "def get_authors(books):\n",
       "    yield from {book.get(\"author\") for book in books if book.get(\"author\")]\n",
       "\n",
       "# Example list of books\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 2\", \"author\": None},\n",
       "    {\"title\": \"Book 3\", \"author\": \"Author C\"}\n",
       "]\n",
       "\n",
       "for author in get_authors(books):\n",
       "    print(author)\n",
       "```\n",
       "\n",
       "This will output:\n",
       "\n",
       "```\n",
       "Author A\n",
       "None\n",
       "Author C\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "def ensure_ollama_ready():\n",
    "    \"\"\"Ensure Ollama is running and ready to use\"\"\"\n",
    "    import subprocess\n",
    "    import time\n",
    "    \n",
    "    # Check if Ollama is running\n",
    "    try:\n",
    "        response = requests.get('http://localhost:11434/api/tags', timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try to start Ollama\n",
    "    try:\n",
    "        print(\"�� Starting Ollama...\")\n",
    "        subprocess.Popen(['ollama', 'serve'], \n",
    "                        stdout=subprocess.DEVNULL, \n",
    "                        stderr=subprocess.DEVNULL)\n",
    "        \n",
    "        # Wait for it to be ready\n",
    "        for i in range(15):\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                response = requests.get('http://localhost:11434/api/tags', timeout=2)\n",
    "                if response.status_code == 200:\n",
    "                    print(\"✅ Ollama is ready!\")\n",
    "                    return True\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(\"❌ Ollama failed to start\")\n",
    "        return False\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ Ollama not found. Please install it first.\")\n",
    "        return False\n",
    "\n",
    "# Use this before your Ollama calls\n",
    "ensure_ollama_ready()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make sure Ollama is running with: ollama run llama3.2\n",
    "response = requests.post('http://localhost:11434/api/generate', json={\n",
    "    'model': 'llama3.2',\n",
    "    'prompt': f\"\"\"You are a helpful assistant that explains code.\n",
    "\n",
    "User question: {question}\n",
    "\n",
    "Please provide a clear explanation:\"\"\",\n",
    "    'stream': True\n",
    "})\n",
    "\n",
    "# Buffer to collect the full response\n",
    "full_response = \"\"\n",
    "\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        data = json.loads(line.decode('utf-8'))\n",
    "        if 'response' in data:\n",
    "            full_response += data['response']\n",
    "            \n",
    "            # Clear previous output and display updated markdown\n",
    "            clear_output(wait=True)\n",
    "            display(Markdown(full_response))\n",
    "            \n",
    "        if data.get('done', False):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b2996fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "# If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "# Gradio UI with Text-to-Speech and Progress\n",
    "import gradio as gr\n",
    "import requests\n",
    "import json\n",
    "import openai\n",
    "import base64\n",
    "import tempfile\n",
    "import os\n",
    "import time\n",
    "\n",
    "# List of available models\n",
    "MODEL_LIST = [\"llama3.2\", \"gpt-4o-mini\"]\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert software engineer and educator with 15+ years of experience in multiple programming languages. Your specialty is making complex code concepts accessible to learners of all levels.\n",
    "\n",
    "**Your Approach:**\n",
    "- Start with a high-level overview of what the code does\n",
    "- Break down complex concepts into digestible parts\n",
    "- Use analogies and real-world examples when helpful\n",
    "- Explain the \"why\" behind design decisions, not just the \"what\"\n",
    "- Point out potential pitfalls or edge cases\n",
    "- Suggest improvements or alternative approaches when relevant\n",
    "\n",
    "**Response Format:**\n",
    "1. **Purpose**: What this code accomplishes\n",
    "2. **How it works**: Step-by-step breakdown\n",
    "3. **Key concepts**: Important programming concepts demonstrated\n",
    "4. **Example**: Show how it would work with sample data\n",
    "5. **Best practices**: Any relevant tips or warnings\n",
    "\n",
    "Always use clear, concise language and structure your explanations with markdown formatting for readability.\"\"\"\n",
    "\n",
    "def stream_ollama_response(model, prompt, system_prompt=SYSTEM_PROMPT):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": f\"{system_prompt}\\n\\nUser question: {prompt}\\n\\nPlease provide a clear explanation:\",\n",
    "        \"stream\": True\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data, stream=True)\n",
    "    full_response = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            data = json.loads(line.decode('utf-8'))\n",
    "            if 'response' in data:\n",
    "                full_response += data['response']\n",
    "                yield full_response\n",
    "            if data.get('done', False):\n",
    "                break\n",
    "\n",
    "def stream_openai_response(model, prompt, system_prompt=SYSTEM_PROMPT):\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not openai.api_key:\n",
    "        raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    full_response = \"\"\n",
    "    for chunk in response:\n",
    "        if hasattr(chunk, \"choices\") and chunk.choices:\n",
    "            delta = chunk.choices[0].delta\n",
    "            if hasattr(delta, \"content\") and delta.content:\n",
    "                full_response += delta.content\n",
    "                yield full_response\n",
    "\n",
    "def stream_response(model, prompt, system_prompt=SYSTEM_PROMPT):\n",
    "    if model == \"llama3.2\":\n",
    "        yield from stream_ollama_response(model, prompt, system_prompt)\n",
    "    elif model == \"gpt-4o-mini\":\n",
    "        yield from stream_openai_response(model, prompt, system_prompt)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model}\")\n",
    "\n",
    "def tts_openai(text, voice=\"alloy\"):\n",
    "    \"\"\"Generate TTS audio and return file path\"\"\"\n",
    "    try:\n",
    "        audio_response = openai.audio.speech.create(\n",
    "            model=\"tts-1\",\n",
    "            voice=voice,\n",
    "            input=text\n",
    "        )\n",
    "        \n",
    "        # Save to temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as f:\n",
    "            f.write(audio_response.content)\n",
    "            return f.name\n",
    "    except Exception as e:\n",
    "        print(f\"TTS Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def gradio_chat(user_message, history, model_choice):\n",
    "    # Compose the chat history into a single prompt\n",
    "    chat_prompt = \"\"\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            chat_prompt += f\"User: {msg['content']}\\n\"\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            chat_prompt += f\"Assistant: {msg['content']}\\n\"\n",
    "    chat_prompt += f\"User: {user_message}\\n\"\n",
    "    \n",
    "    # Stream the response\n",
    "    response_stream = stream_response(model_choice, chat_prompt)\n",
    "    partial = \"\"\n",
    "    for partial in response_stream:\n",
    "        yield partial\n",
    "\n",
    "def respond(user_message, history, model_choice, enable_tts, progress=gr.Progress()):\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    # Stream the response\n",
    "    response = \"\"\n",
    "    for partial in gradio_chat(user_message, history, model_choice):\n",
    "        response = partial\n",
    "        current_messages = []\n",
    "        for msg in history:\n",
    "            current_messages.append(msg)\n",
    "        current_messages.extend([\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ])\n",
    "        yield current_messages, None\n",
    "    \n",
    "    # Generate TTS if enabled and using OpenAI\n",
    "    audio_file = None\n",
    "    if enable_tts and model_choice == \"gpt-4o-mini\":\n",
    "        # Show progress for audio generation\n",
    "        progress(0, desc=\"Starting audio generation...\")\n",
    "        time.sleep(0.1)  # Small delay to show progress\n",
    "        \n",
    "        progress(0.3, desc=\"Converting text to speech...\")\n",
    "        audio_file = tts_openai(response)\n",
    "        \n",
    "        progress(0.7, desc=\"Processing audio...\")\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        progress(1.0, desc=\"Audio ready!\")\n",
    "    \n",
    "    # Final yield with complete messages\n",
    "    final_messages = []\n",
    "    for msg in history:\n",
    "        final_messages.append(msg)\n",
    "    final_messages.extend([\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ])\n",
    "    yield final_messages, audio_file\n",
    "\n",
    "def restart_chat():\n",
    "    return [], [], None\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## LLM Chat with Text-to-Speech\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        model_choice = gr.Dropdown(\n",
    "            choices=MODEL_LIST, \n",
    "            value=MODEL_LIST[0], \n",
    "            label=\"LLM Model\"\n",
    "        )\n",
    "        enable_tts = gr.Checkbox(\n",
    "            label=\"Enable Text-to-Speech (OpenAI only)\", \n",
    "            value=False\n",
    "        )\n",
    "    \n",
    "    chatbot = gr.Chatbot(\n",
    "        label=\"Chat\", \n",
    "        show_copy_button=True, \n",
    "        render_markdown=True,\n",
    "        type=\"messages\"\n",
    "    )\n",
    "    \n",
    "    audio_output = gr.Audio(\n",
    "        label=\"Generated Audio\", \n",
    "        type=\"filepath\",\n",
    "        visible=True\n",
    "    )\n",
    "    \n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(label=\"Your message\", scale=4)\n",
    "        send_btn = gr.Button(\"Send\", scale=1)\n",
    "        restart_btn = gr.Button(\"Restart\", scale=1)\n",
    "    \n",
    "    state = gr.State([])\n",
    "\n",
    "    send_btn.click(\n",
    "        respond,\n",
    "        inputs=[msg, chatbot, model_choice, enable_tts],\n",
    "        outputs=[chatbot, audio_output],\n",
    "        queue=True\n",
    "    )\n",
    "    \n",
    "    msg.submit(\n",
    "        respond,\n",
    "        inputs=[msg, chatbot, model_choice, enable_tts],\n",
    "        outputs=[chatbot, audio_output],\n",
    "        queue=True\n",
    "    )\n",
    "\n",
    "    restart_btn.click(\n",
    "        restart_chat,\n",
    "        inputs=None,\n",
    "        outputs=[chatbot, state, audio_output],\n",
    "        queue=False\n",
    "    )\n",
    "\n",
    "demo.launch()\n",
    "\n",
    "\n",
    "# Tool - tbd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf057746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
